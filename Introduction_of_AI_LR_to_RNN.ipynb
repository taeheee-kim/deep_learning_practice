{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taeheee-kim/deep_learning_practice/blob/master/Introduction_of_AI_LR_to_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnRxIQBJvfB8"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCBMd6Y7rbfO",
        "outputId": "52e09f4c-ed5c-4b3b-daf2-136d6f2c8082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch    0/1000 Cost: 0.693147\n",
            "Epoch  100/1000 Cost: 0.134722\n",
            "Epoch  200/1000 Cost: 0.080643\n",
            "Epoch  300/1000 Cost: 0.057900\n",
            "Epoch  400/1000 Cost: 0.045300\n",
            "Epoch  500/1000 Cost: 0.037261\n",
            "Epoch  600/1000 Cost: 0.031673\n",
            "Epoch  700/1000 Cost: 0.027556\n",
            "Epoch  800/1000 Cost: 0.024394\n",
            "Epoch  900/1000 Cost: 0.021888\n",
            "Epoch 1000/1000 Cost: 0.019852\n",
            "tensor([[False],\n",
            "        [False],\n",
            "        [False],\n",
            "        [ True],\n",
            "        [ True],\n",
            "        [ True]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "x_data = [[1,2],\n",
        "          [2,3],\n",
        "          [3,1],\n",
        "          [4,3],\n",
        "          [5,3],\n",
        "          [6,2]]\n",
        "\n",
        "y_data = [[0],\n",
        "          [0],\n",
        "          [0],\n",
        "          [1],\n",
        "          [1],\n",
        "          [1]]\n",
        "\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)\n",
        "\n",
        "W = torch.zeros((2, 1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "optimizer = optim.SGD([W,b], lr=1)\n",
        "\n",
        "nb_epochs = 1000\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
        "    cost = -(y_train * torch.log(hypothesis) + (1 - y_train) * torch.log(1 - hypothesis)).mean()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))\n",
        "\n",
        "if epoch == nb_epochs:\n",
        "    prediction = hypothesis >= torch.FloatTensor([0.5])\n",
        "    print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pttGiTym2TYL"
      },
      "source": [
        "## Logistic Regression with NN Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P05IabI4zRtj",
        "outputId": "61c821fb-8076-417d-ab43-240298c44b8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch    0/1000 Cost: 0.782701 Accuracy 50.00%\n",
            "Epoch   10/1000 Cost: 0.455712 Accuracy 66.67%\n",
            "Epoch   20/1000 Cost: 0.464168 Accuracy 66.67%\n",
            "Epoch   30/1000 Cost: 0.390357 Accuracy 83.33%\n",
            "Epoch   40/1000 Cost: 0.331176 Accuracy 83.33%\n",
            "Epoch   50/1000 Cost: 0.279423 Accuracy 83.33%\n",
            "Epoch   60/1000 Cost: 0.232532 Accuracy 100.00%\n",
            "Epoch   70/1000 Cost: 0.191559 Accuracy 100.00%\n",
            "Epoch   80/1000 Cost: 0.162585 Accuracy 100.00%\n",
            "Epoch   90/1000 Cost: 0.147155 Accuracy 100.00%\n",
            "Epoch  100/1000 Cost: 0.136820 Accuracy 100.00%\n",
            "Epoch  110/1000 Cost: 0.127995 Accuracy 100.00%\n",
            "Epoch  120/1000 Cost: 0.120259 Accuracy 100.00%\n",
            "Epoch  130/1000 Cost: 0.113423 Accuracy 100.00%\n",
            "Epoch  140/1000 Cost: 0.107336 Accuracy 100.00%\n",
            "Epoch  150/1000 Cost: 0.101884 Accuracy 100.00%\n",
            "Epoch  160/1000 Cost: 0.096970 Accuracy 100.00%\n",
            "Epoch  170/1000 Cost: 0.092520 Accuracy 100.00%\n",
            "Epoch  180/1000 Cost: 0.088470 Accuracy 100.00%\n",
            "Epoch  190/1000 Cost: 0.084768 Accuracy 100.00%\n",
            "Epoch  200/1000 Cost: 0.081371 Accuracy 100.00%\n",
            "Epoch  210/1000 Cost: 0.078243 Accuracy 100.00%\n",
            "Epoch  220/1000 Cost: 0.075352 Accuracy 100.00%\n",
            "Epoch  230/1000 Cost: 0.072672 Accuracy 100.00%\n",
            "Epoch  240/1000 Cost: 0.070181 Accuracy 100.00%\n",
            "Epoch  250/1000 Cost: 0.067859 Accuracy 100.00%\n",
            "Epoch  260/1000 Cost: 0.065690 Accuracy 100.00%\n",
            "Epoch  270/1000 Cost: 0.063658 Accuracy 100.00%\n",
            "Epoch  280/1000 Cost: 0.061751 Accuracy 100.00%\n",
            "Epoch  290/1000 Cost: 0.059958 Accuracy 100.00%\n",
            "Epoch  300/1000 Cost: 0.058268 Accuracy 100.00%\n",
            "Epoch  310/1000 Cost: 0.056673 Accuracy 100.00%\n",
            "Epoch  320/1000 Cost: 0.055165 Accuracy 100.00%\n",
            "Epoch  330/1000 Cost: 0.053736 Accuracy 100.00%\n",
            "Epoch  340/1000 Cost: 0.052382 Accuracy 100.00%\n",
            "Epoch  350/1000 Cost: 0.051095 Accuracy 100.00%\n",
            "Epoch  360/1000 Cost: 0.049872 Accuracy 100.00%\n",
            "Epoch  370/1000 Cost: 0.048707 Accuracy 100.00%\n",
            "Epoch  380/1000 Cost: 0.047596 Accuracy 100.00%\n",
            "Epoch  390/1000 Cost: 0.046535 Accuracy 100.00%\n",
            "Epoch  400/1000 Cost: 0.045522 Accuracy 100.00%\n",
            "Epoch  410/1000 Cost: 0.044553 Accuracy 100.00%\n",
            "Epoch  420/1000 Cost: 0.043625 Accuracy 100.00%\n",
            "Epoch  430/1000 Cost: 0.042735 Accuracy 100.00%\n",
            "Epoch  440/1000 Cost: 0.041882 Accuracy 100.00%\n",
            "Epoch  450/1000 Cost: 0.041062 Accuracy 100.00%\n",
            "Epoch  460/1000 Cost: 0.040275 Accuracy 100.00%\n",
            "Epoch  470/1000 Cost: 0.039518 Accuracy 100.00%\n",
            "Epoch  480/1000 Cost: 0.038789 Accuracy 100.00%\n",
            "Epoch  490/1000 Cost: 0.038087 Accuracy 100.00%\n",
            "Epoch  500/1000 Cost: 0.037410 Accuracy 100.00%\n",
            "Epoch  510/1000 Cost: 0.036758 Accuracy 100.00%\n",
            "Epoch  520/1000 Cost: 0.036128 Accuracy 100.00%\n",
            "Epoch  530/1000 Cost: 0.035519 Accuracy 100.00%\n",
            "Epoch  540/1000 Cost: 0.034931 Accuracy 100.00%\n",
            "Epoch  550/1000 Cost: 0.034363 Accuracy 100.00%\n",
            "Epoch  560/1000 Cost: 0.033813 Accuracy 100.00%\n",
            "Epoch  570/1000 Cost: 0.033280 Accuracy 100.00%\n",
            "Epoch  580/1000 Cost: 0.032764 Accuracy 100.00%\n",
            "Epoch  590/1000 Cost: 0.032264 Accuracy 100.00%\n",
            "Epoch  600/1000 Cost: 0.031780 Accuracy 100.00%\n",
            "Epoch  610/1000 Cost: 0.031310 Accuracy 100.00%\n",
            "Epoch  620/1000 Cost: 0.030854 Accuracy 100.00%\n",
            "Epoch  630/1000 Cost: 0.030411 Accuracy 100.00%\n",
            "Epoch  640/1000 Cost: 0.029980 Accuracy 100.00%\n",
            "Epoch  650/1000 Cost: 0.029562 Accuracy 100.00%\n",
            "Epoch  660/1000 Cost: 0.029156 Accuracy 100.00%\n",
            "Epoch  670/1000 Cost: 0.028760 Accuracy 100.00%\n",
            "Epoch  680/1000 Cost: 0.028376 Accuracy 100.00%\n",
            "Epoch  690/1000 Cost: 0.028001 Accuracy 100.00%\n",
            "Epoch  700/1000 Cost: 0.027637 Accuracy 100.00%\n",
            "Epoch  710/1000 Cost: 0.027282 Accuracy 100.00%\n",
            "Epoch  720/1000 Cost: 0.026936 Accuracy 100.00%\n",
            "Epoch  730/1000 Cost: 0.026599 Accuracy 100.00%\n",
            "Epoch  740/1000 Cost: 0.026270 Accuracy 100.00%\n",
            "Epoch  750/1000 Cost: 0.025949 Accuracy 100.00%\n",
            "Epoch  760/1000 Cost: 0.025636 Accuracy 100.00%\n",
            "Epoch  770/1000 Cost: 0.025331 Accuracy 100.00%\n",
            "Epoch  780/1000 Cost: 0.025033 Accuracy 100.00%\n",
            "Epoch  790/1000 Cost: 0.024742 Accuracy 100.00%\n",
            "Epoch  800/1000 Cost: 0.024458 Accuracy 100.00%\n",
            "Epoch  810/1000 Cost: 0.024180 Accuracy 100.00%\n",
            "Epoch  820/1000 Cost: 0.023908 Accuracy 100.00%\n",
            "Epoch  830/1000 Cost: 0.023643 Accuracy 100.00%\n",
            "Epoch  840/1000 Cost: 0.023383 Accuracy 100.00%\n",
            "Epoch  850/1000 Cost: 0.023129 Accuracy 100.00%\n",
            "Epoch  860/1000 Cost: 0.022881 Accuracy 100.00%\n",
            "Epoch  870/1000 Cost: 0.022638 Accuracy 100.00%\n",
            "Epoch  880/1000 Cost: 0.022400 Accuracy 100.00%\n",
            "Epoch  890/1000 Cost: 0.022167 Accuracy 100.00%\n",
            "Epoch  900/1000 Cost: 0.021939 Accuracy 100.00%\n",
            "Epoch  910/1000 Cost: 0.021716 Accuracy 100.00%\n",
            "Epoch  920/1000 Cost: 0.021497 Accuracy 100.00%\n",
            "Epoch  930/1000 Cost: 0.021282 Accuracy 100.00%\n",
            "Epoch  940/1000 Cost: 0.021072 Accuracy 100.00%\n",
            "Epoch  950/1000 Cost: 0.020866 Accuracy 100.00%\n",
            "Epoch  960/1000 Cost: 0.020664 Accuracy 100.00%\n",
            "Epoch  970/1000 Cost: 0.020466 Accuracy 100.00%\n",
            "Epoch  980/1000 Cost: 0.020272 Accuracy 100.00%\n",
            "Epoch  990/1000 Cost: 0.020081 Accuracy 100.00%\n",
            "Epoch 1000/1000 Cost: 0.019894 Accuracy 100.00%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(2,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "x_data = [[1,2],\n",
        "          [2,3],\n",
        "          [3,1],\n",
        "          [4,3],\n",
        "          [5,3],\n",
        "          [6,2]]\n",
        "\n",
        "y_data = [[0],\n",
        "          [0],\n",
        "          [0],\n",
        "          [1],\n",
        "          [1],\n",
        "          [1]]\n",
        "\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)\n",
        "\n",
        "model = BinaryClassifier()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=1)\n",
        "\n",
        "nb_epochs = 1000\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    hypothesis = model(x_train)\n",
        "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        prediction = hypothesis >= torch.FloatTensor([0.5])\n",
        "        correct_prediction = prediction.float() == y_train\n",
        "        accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format(epoch, nb_epochs, cost.item(), accuracy * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSWmyGfe4iuM"
      },
      "source": [
        "## Softmax Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QyTwTwv2Ssu",
        "outputId": "943f01c3-30a1-4416-fef7-f2e1f00bfb99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch    0/1000 Cost: 0.549306\n",
            "Epoch  100/1000 Cost: 0.345172\n",
            "Epoch  200/1000 Cost: 0.307202\n",
            "Epoch  300/1000 Cost: 0.285251\n",
            "Epoch  400/1000 Cost: 0.269813\n",
            "Epoch  500/1000 Cost: 0.257785\n",
            "Epoch  600/1000 Cost: 0.247828\n",
            "Epoch  700/1000 Cost: 0.239258\n",
            "Epoch  800/1000 Cost: 0.231684\n",
            "Epoch  900/1000 Cost: 0.224862\n",
            "Epoch 1000/1000 Cost: 0.218631\n"
          ]
        }
      ],
      "source": [
        "#learning rate = 0.5\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "x_train = [[1, 2, 1, 1],\n",
        "           [2, 1, 3, 2],\n",
        "           [3, 1, 3, 4],\n",
        "           [4, 1, 5, 5],\n",
        "           [1, 7, 5, 5],\n",
        "           [1, 2, 5, 6],\n",
        "           [1, 6, 6, 6],\n",
        "           [1, 7, 7, 7]]\n",
        "\n",
        "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
        "x_train = torch.FloatTensor(x_train)\n",
        "y_train = torch.LongTensor(y_train)\n",
        "\n",
        "y_one_hot = torch.zeros(8, 3)\n",
        "y_one_hot.scatter_(1, y_train.unsqueeze(1), 0.5)\n",
        "\n",
        "W = torch.zeros((4, 3), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "optimizer = optim.SGD([W, b], lr=0.1)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    hypothesis = F.softmax(x_train.matmul(W) + b, dim=1)\n",
        "    cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pvhnZo52Sjh",
        "outputId": "717b564f-c945-4f8a-c7ee-68e2d9a7fdef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch    0/1000 Cost: 0.109861\n",
            "Epoch  100/1000 Cost: 0.085082\n",
            "Epoch  200/1000 Cost: 0.078491\n",
            "Epoch  300/1000 Cost: 0.074459\n",
            "Epoch  400/1000 Cost: 0.071465\n",
            "Epoch  500/1000 Cost: 0.069069\n",
            "Epoch  600/1000 Cost: 0.067078\n",
            "Epoch  700/1000 Cost: 0.065383\n",
            "Epoch  800/1000 Cost: 0.063912\n",
            "Epoch  900/1000 Cost: 0.062618\n",
            "Epoch 1000/1000 Cost: 0.061464\n"
          ]
        }
      ],
      "source": [
        "#learning rate = 0.1\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "x_train = [[1, 2, 1, 1],\n",
        "           [2, 1, 3, 2],\n",
        "           [3, 1, 3, 4],\n",
        "           [4, 1, 5, 5],\n",
        "           [1, 7, 5, 5],\n",
        "           [1, 2, 5, 6],\n",
        "           [1, 6, 6, 6],\n",
        "           [1, 7, 7, 7]]\n",
        "\n",
        "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
        "x_train = torch.FloatTensor(x_train)\n",
        "y_train = torch.LongTensor(y_train)\n",
        "\n",
        "y_one_hot = torch.zeros(8, 3)\n",
        "y_one_hot.scatter_(1, y_train.unsqueeze(1), 0.1)\n",
        "\n",
        "W = torch.zeros((4, 3), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "optimizer = optim.SGD([W, b], lr=0.1)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    hypothesis = F.softmax(x_train.matmul(W) + b, dim=1)\n",
        "    cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsWQfTYP70t1"
      },
      "source": [
        "## Softmax Classification with NN Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeiBdVUX7jQW",
        "outputId": "d387b2c0-8a89-4cf1-f130-80c103b75b7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch    0/1000 Cost: 2.865484\n",
            "Epoch  100/1000 Cost: 0.331276\n",
            "Epoch  200/1000 Cost: 0.214730\n",
            "Epoch  300/1000 Cost: 0.147977\n",
            "Epoch  400/1000 Cost: 0.108013\n",
            "Epoch  500/1000 Cost: 0.082336\n",
            "Epoch  600/1000 Cost: 0.064835\n",
            "Epoch  700/1000 Cost: 0.052357\n",
            "Epoch  800/1000 Cost: 0.043142\n",
            "Epoch  900/1000 Cost: 0.036142\n",
            "Epoch 1000/1000 Cost: 0.030699\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class SoftmaxClassifierModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(4, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "x_train = [[1, 2, 1, 1],\n",
        "           [2, 1, 3, 2],\n",
        "           [3, 1, 3, 4],\n",
        "           [4, 1, 5, 5],\n",
        "           [1, 7, 5, 5],\n",
        "           [1, 2, 5, 6],\n",
        "           [1, 6, 6, 6],\n",
        "           [1, 7, 7, 7]]\n",
        "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
        "x_train = torch.FloatTensor(x_train)\n",
        "y_train = torch.LongTensor(y_train)\n",
        "\n",
        "y_one_hot = torch.zeros(8, 3)\n",
        "y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
        "\n",
        "model = SoftmaxClassifierModel()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    prediction = model(x_train)\n",
        "    cost = F.cross_entropy(prediction, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRexl9Jh89UB"
      },
      "source": [
        "## Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "id": "7a815fQ_8cUN",
        "outputId": "e370ee77-89cc-44af-a00e-8007504828e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "총 배치의 수 : 600\n",
            "[Epoch:    1] cost = 0.265549093\n",
            "[Epoch:    2] cost = 0.0955918431\n",
            "[Epoch:    3] cost = 0.0623133369\n",
            "[Epoch:    4] cost = 0.042559851\n",
            "[Epoch:    5] cost = 0.0322850607\n",
            "[Epoch:    6] cost = 0.0237160753\n",
            "[Epoch:    7] cost = 0.0210654642\n",
            "[Epoch:    8] cost = 0.0186767541\n",
            "[Epoch:    9] cost = 0.0147612207\n",
            "[Epoch:   10] cost = 0.0115683805\n",
            "[Epoch:   11] cost = 0.0137381619\n",
            "[Epoch:   12] cost = 0.0113736456\n",
            "[Epoch:   13] cost = 0.00864754803\n",
            "[Epoch:   14] cost = 0.0123699969\n",
            "[Epoch:   15] cost = 0.0100851152\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:81: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:71: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9768000245094299\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.init\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class DNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.fc1(x.view(-1, 784)))\n",
        "        out = self.relu(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "model = DNN()\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "total_batch = len(data_loader)\n",
        "print('총 배치의 수 : {}'.format(total_batch))\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    X_test = mnist_test.test_data.view(len(mnist_test), 1 * 28 * 28).float()\n",
        "    Y_test = mnist_test.test_labels\n",
        "\n",
        "    prediction = model(X_test)\n",
        "    prediction = torch.argmax(prediction, 1)\n",
        "    correct_prediction = prediction == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    fig = plt.figure()\n",
        "    for i in range(10):\n",
        "        subplot = fig.add_subplot(2, 5, i + 1)\n",
        "\n",
        "        subplot.set_xticks([])\n",
        "        subplot.set_yticks([])\n",
        "\n",
        "        subplot.set_title('%d' % prediction[i])\n",
        "        subplot.imshow(X_test[i].reshape((28, 28)),\n",
        "                       cmap=plt.cm.gray_r)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yif8UycX9zHl"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjkBztt_-tFX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.init\n",
        "\n",
        "class CNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        self.layer2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        self.fc = torch.nn.Linear(7 * 7 * 64, 10, bias=True)\n",
        "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0), -1)  # 전결합층을 위해서 Flatten\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YXmHVGz9ofH",
        "outputId": "529446e8-a18a-4d98-c96d-2863477e6122"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "총 배치의 수 : 600\n",
            "[Epoch:    1] cost = 0.217296466\n",
            "[Epoch:    2] cost = 0.0576543435\n",
            "[Epoch:    3] cost = 0.0427408144\n",
            "[Epoch:    4] cost = 0.0344967134\n",
            "[Epoch:    5] cost = 0.0285993256\n",
            "[Epoch:    6] cost = 0.0247306935\n",
            "[Epoch:    7] cost = 0.0219664965\n",
            "[Epoch:    8] cost = 0.0175395068\n",
            "[Epoch:    9] cost = 0.0155572006\n",
            "[Epoch:   10] cost = 0.0130234016\n",
            "[Epoch:   11] cost = 0.0111826826\n",
            "[Epoch:   12] cost = 0.00940733682\n",
            "[Epoch:   13] cost = 0.00759225991\n",
            "[Epoch:   14] cost = 0.00745917019\n",
            "[Epoch:   15] cost = 0.007038143\n",
            "Accuracy: 0.98580002784729\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "model = CNN().to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "total_batch = len(data_loader)\n",
        "print('총 배치의 수 : {}'.format(total_batch))\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader:\n",
        "        # image is already size of (28x28), no reshape\n",
        "        # label is not one-hot encoded\n",
        "        X = X.to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))\n",
        "\n",
        "with torch.no_grad():\n",
        "    X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\n",
        "    Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "    prediction = model(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu7SthEo_QhQ"
      },
      "source": [
        "## RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDtjDODr_Ob7"
      },
      "outputs": [],
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUFawl3rBWn3",
        "outputId": "b1753a5e-5869-4f0a-b1c0-03bae87565a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문자 집합의 크기 : 5\n",
            "{'e': 0, 'h': 1, 'i': 2, 'l': 3, 'o': 4}\n",
            "{0: 'e', 1: 'h', 2: 'i', 3: 'l', 4: 'o'}\n",
            "[1, 2, 1, 0, 3, 3]\n",
            "[2, 1, 0, 3, 3, 4]\n",
            "[[1, 2, 1, 0, 3, 3]]\n",
            "[[2, 1, 0, 3, 3, 4]]\n",
            "[array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 1., 0., 0.],\n",
            "       [0., 1., 0., 0., 0.],\n",
            "       [1., 0., 0., 0., 0.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 0., 0., 1., 0.]])]\n",
            "훈련 데이터의 크기 : torch.Size([1, 6, 5])\n",
            "레이블의 크기 : torch.Size([1, 6])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-20-3fff8f66f616>:36: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  X = torch.FloatTensor(x_one_hot)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "input_str = 'hihell'\n",
        "label_str = 'ihello'\n",
        "char_vocab = sorted(list(set(input_str+label_str)))\n",
        "vocab_size = len(char_vocab)\n",
        "print ('문자 집합의 크기 : {}'.format(vocab_size))\n",
        "\n",
        "input_size = vocab_size # 입력의 크기 = 문자 집합의 크기\n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "learning_rate = 0.1\n",
        "\n",
        "char_to_index = dict((c, i) for i, c in enumerate(char_vocab))\n",
        "print(char_to_index)\n",
        "\n",
        "index_to_char={}\n",
        "for key, value in char_to_index.items():\n",
        "    index_to_char[value] = key\n",
        "print(index_to_char)\n",
        "\n",
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in label_str]\n",
        "print(x_data)\n",
        "print(y_data)\n",
        "\n",
        "\n",
        "x_data = [x_data]\n",
        "y_data = [y_data]\n",
        "print(x_data)\n",
        "print(y_data)\n",
        "\n",
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "print(x_one_hot)\n",
        "\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)\n",
        "\n",
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_EmJZIYBgfJ",
        "outputId": "e60340aa-6541-4eb6-8558-008ac88b297a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 loss:  1.5443462133407593 prediction:  [[0 3 0 3 4 3]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  elelol\n",
            "1 loss:  1.3840551376342773 prediction:  [[3 1 3 3 3 3]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  lhllll\n",
            "2 loss:  1.2196571826934814 prediction:  [[3 1 0 3 3 3]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  lhelll\n",
            "3 loss:  1.0671108961105347 prediction:  [[0 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ehello\n",
            "4 loss:  0.9101631045341492 prediction:  [[0 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ehello\n",
            "5 loss:  0.7525632977485657 prediction:  [[0 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ehello\n",
            "6 loss:  0.6184913516044617 prediction:  [[0 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ehello\n",
            "7 loss:  0.4966677725315094 prediction:  [[0 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ehello\n",
            "8 loss:  0.3975163996219635 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "9 loss:  0.31858769059181213 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "10 loss:  0.2481779307126999 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "11 loss:  0.18872125446796417 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "12 loss:  0.1428099125623703 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "13 loss:  0.1058088168501854 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "14 loss:  0.07640156894922256 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "15 loss:  0.05555577576160431 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "16 loss:  0.04151445999741554 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "17 loss:  0.03148985654115677 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "18 loss:  0.023866845294833183 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "19 loss:  0.018104523420333862 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "20 loss:  0.013906605541706085 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "21 loss:  0.010917332954704762 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "22 loss:  0.008786074817180634 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "23 loss:  0.00723862973973155 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "24 loss:  0.006084250286221504 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "25 loss:  0.005196876358240843 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "26 loss:  0.004494968336075544 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "27 loss:  0.003926561679691076 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "28 loss:  0.003458488965407014 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "29 loss:  0.003069037338718772 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "30 loss:  0.0027431396301835775 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "31 loss:  0.0024696108885109425 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "32 loss:  0.0022395418491214514 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "33 loss:  0.0020454914774745703 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "34 loss:  0.0018814554205164313 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "35 loss:  0.0017421358497813344 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "36 loss:  0.0016233599744737148 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "37 loss:  0.0015215851599350572 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "38 loss:  0.0014340403722599149 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "39 loss:  0.0013582106912508607 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "40 loss:  0.0012922139139845967 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "41 loss:  0.001234544673934579 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "42 loss:  0.0011838158825412393 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "43 loss:  0.0011390561703592539 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "44 loss:  0.0010992944007739425 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "45 loss:  0.0010639360407367349 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "46 loss:  0.0010321479057893157 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "47 loss:  0.0010035932064056396 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "48 loss:  0.0009777563391253352 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "49 loss:  0.0009543595369905233 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "50 loss:  0.0009330259636044502 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "51 loss:  0.0009135773871093988 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "52 loss:  0.0008956564124673605 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "53 loss:  0.0008792434819042683 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "54 loss:  0.0008640409796498716 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "55 loss:  0.0008499692776240408 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "56 loss:  0.0008368701674044132 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "57 loss:  0.000824604241643101 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "58 loss:  0.0008132115472108126 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "59 loss:  0.0008024537819437683 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "60 loss:  0.0007923906086944044 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "61 loss:  0.0007828830857761204 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "62 loss:  0.0007739312131889164 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "63 loss:  0.0007654358050785959 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "64 loss:  0.0007573968614451587 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "65 loss:  0.0007497151382267475 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "66 loss:  0.0007424105424433947 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "67 loss:  0.000735423294827342 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "68 loss:  0.0007287338376045227 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "69 loss:  0.0007223421707749367 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "70 loss:  0.0007162282709032297 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "71 loss:  0.0007102929521352053 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "72 loss:  0.0007045958773232996 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "73 loss:  0.0006990774418227375 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "74 loss:  0.0006937573780305684 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "75 loss:  0.0006885763141326606 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "76 loss:  0.0006835737731307745 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "77 loss:  0.0006787499296478927 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "78 loss:  0.0006740053649991751 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "79 loss:  0.0006694197654724121 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "80 loss:  0.000664933177176863 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "81 loss:  0.0006605261587537825 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "82 loss:  0.0006562381167896092 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "83 loss:  0.0006520692259073257 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "84 loss:  0.0006479399744421244 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "85 loss:  0.000643890060018748 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "86 loss:  0.0006399196572601795 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "87 loss:  0.0006360483821481466 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "88 loss:  0.0006322168046608567 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "89 loss:  0.0006284845294430852 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "90 loss:  0.0006247521960176528 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "91 loss:  0.0006211588042788208 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "92 loss:  0.0006175455055199564 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "93 loss:  0.0006139720208011568 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "94 loss:  0.0006104579078964889 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "95 loss:  0.0006070034578442574 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "96 loss:  0.0006036084378138185 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "97 loss:  0.0006002532318234444 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "98 loss:  0.0005968781188130379 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n",
            "99 loss:  0.0005936021916568279 prediction:  [[2 1 0 3 3 4]] true Y:  [[2, 1, 0, 3, 3, 4]] prediction str:  ihello\n"
          ]
        }
      ],
      "source": [
        "net = Net(input_size, hidden_size, output_size)\n",
        "outputs = net(X)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
        "\n",
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    loss = criterion(outputs.view(-1, input_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    result = outputs.data.numpy().argmax(axis=2)\n",
        "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
        "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr2CL97EBus7",
        "outputId": "583edbb8-9318-47a0-d30b-ebe1b63c2fd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'o': 0, 'p': 1, 'm': 2, 'g': 3, 'b': 4, 'h': 5, 'l': 6, ',': 7, 'k': 8, '.': 9, 's': 10, 'y': 11, \"'\": 12, 'c': 13, 'r': 14, 'a': 15, 'f': 16, 'i': 17, 't': 18, 'u': 19, 'd': 20, ' ': 21, 'w': 22, 'n': 23, 'e': 24}\n",
            "문자 집합의 크기 : 25\n",
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether to \n",
            "52 gether to  -> ether to c\n",
            "53 ether to c -> ther to co\n",
            "54 ther to co -> her to col\n",
            "55 her to col -> er to coll\n",
            "56 er to coll -> r to colle\n",
            "57 r to colle ->  to collec\n",
            "58  to collec -> to collect\n",
            "59 to collect -> o collect \n",
            "60 o collect  ->  collect w\n",
            "61  collect w -> collect wo\n",
            "62 collect wo -> ollect woo\n",
            "63 ollect woo -> llect wood\n",
            "64 llect wood -> lect wood \n",
            "65 lect wood  -> ect wood a\n",
            "66 ect wood a -> ct wood an\n",
            "67 ct wood an -> t wood and\n",
            "68 t wood and ->  wood and \n",
            "69  wood and  -> wood and d\n",
            "70 wood and d -> ood and do\n",
            "71 ood and do -> od and don\n",
            "72 od and don -> d and don'\n",
            "73 d and don' ->  and don't\n",
            "74  and don't -> and don't \n",
            "75 and don't  -> nd don't a\n",
            "76 nd don't a -> d don't as\n",
            "77 d don't as ->  don't ass\n",
            "78  don't ass -> don't assi\n",
            "79 don't assi -> on't assig\n",
            "80 on't assig -> n't assign\n",
            "81 n't assign -> 't assign \n",
            "82 't assign  -> t assign t\n",
            "83 t assign t ->  assign th\n",
            "84  assign th -> assign the\n",
            "85 assign the -> ssign them\n",
            "86 ssign them -> sign them \n",
            "87 sign them  -> ign them t\n",
            "88 ign them t -> gn them ta\n",
            "89 gn them ta -> n them tas\n",
            "90 n them tas ->  them task\n",
            "91  them task -> them tasks\n",
            "92 them tasks -> hem tasks \n",
            "93 hem tasks  -> em tasks a\n",
            "94 em tasks a -> m tasks an\n",
            "95 m tasks an ->  tasks and\n",
            "96  tasks and -> tasks and \n",
            "97 tasks and  -> asks and w\n",
            "98 asks and w -> sks and wo\n",
            "99 sks and wo -> ks and wor\n",
            "100 ks and wor -> s and work\n",
            "101 s and work ->  and work,\n",
            "102  and work, -> and work, \n",
            "103 and work,  -> nd work, b\n",
            "104 nd work, b -> d work, bu\n",
            "105 d work, bu ->  work, but\n",
            "106  work, but -> work, but \n",
            "107 work, but  -> ork, but r\n",
            "108 ork, but r -> rk, but ra\n",
            "109 rk, but ra -> k, but rat\n",
            "110 k, but rat -> , but rath\n",
            "111 , but rath ->  but rathe\n",
            "112  but rathe -> but rather\n",
            "113 but rather -> ut rather \n",
            "114 ut rather  -> t rather t\n",
            "115 t rather t ->  rather te\n",
            "116  rather te -> rather tea\n",
            "117 rather tea -> ather teac\n",
            "118 ather teac -> ther teach\n",
            "119 ther teach -> her teach \n",
            "120 her teach  -> er teach t\n",
            "121 er teach t -> r teach th\n",
            "122 r teach th ->  teach the\n",
            "123  teach the -> teach them\n",
            "124 teach them -> each them \n",
            "125 each them  -> ach them t\n",
            "126 ach them t -> ch them to\n",
            "127 ch them to -> h them to \n",
            "128 h them to  ->  them to l\n",
            "129  them to l -> them to lo\n",
            "130 them to lo -> hem to lon\n",
            "131 hem to lon -> em to long\n",
            "132 em to long -> m to long \n",
            "133 m to long  ->  to long f\n",
            "134  to long f -> to long fo\n",
            "135 to long fo -> o long for\n",
            "136 o long for ->  long for \n",
            "137  long for  -> long for t\n",
            "138 long for t -> ong for th\n",
            "139 ong for th -> ng for the\n",
            "140 ng for the -> g for the \n",
            "141 g for the  ->  for the e\n",
            "142  for the e -> for the en\n",
            "143 for the en -> or the end\n",
            "144 or the end -> r the endl\n",
            "145 r the endl ->  the endle\n",
            "146  the endle -> the endles\n",
            "147 the endles -> he endless\n",
            "148 he endless -> e endless \n",
            "149 e endless  ->  endless i\n",
            "150  endless i -> endless im\n",
            "151 endless im -> ndless imm\n",
            "152 ndless imm -> dless imme\n",
            "153 dless imme -> less immen\n",
            "154 less immen -> ess immens\n",
            "155 ess immens -> ss immensi\n",
            "156 ss immensi -> s immensit\n",
            "157 s immensit ->  immensity\n",
            "158  immensity -> immensity \n",
            "159 immensity  -> mmensity o\n",
            "160 mmensity o -> mensity of\n",
            "161 mensity of -> ensity of \n",
            "162 ensity of  -> nsity of t\n",
            "163 nsity of t -> sity of th\n",
            "164 sity of th -> ity of the\n",
            "165 ity of the -> ty of the \n",
            "166 ty of the  -> y of the s\n",
            "167 y of the s ->  of the se\n",
            "168  of the se -> of the sea\n",
            "169 of the sea -> f the sea.\n",
            "[17, 16, 21, 11, 0, 19, 21, 22, 15, 23]\n",
            "[16, 21, 11, 0, 19, 21, 22, 15, 23, 18]\n",
            "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
            "레이블의 크기 : torch.Size([170, 10])\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 1., 0.]])\n"
          ]
        }
      ],
      "source": [
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")\n",
        "\n",
        "char_set = list(set(sentence))\n",
        "char_dic = {c: i for i, c in enumerate(char_set)}\n",
        "\n",
        "print(char_dic)\n",
        "dic_size = len(char_dic)\n",
        "print('문자 집합의 크기 : {}'.format(dic_size))\n",
        "\n",
        "hidden_size = dic_size\n",
        "sequence_length = 10  # 임의 숫자 지정\n",
        "learning_rate = 0.1\n",
        "\n",
        "# 데이터 구성\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "    x_str = sentence[i:i + sequence_length]\n",
        "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
        "    print(i, x_str, '->', y_str)\n",
        "\n",
        "    x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
        "    y_data.append([char_dic[c] for c in y_str])  # y str to index\n",
        "\n",
        "print(x_data[0])\n",
        "print(y_data[0])\n",
        "\n",
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)\n",
        "\n",
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))\n",
        "\n",
        "print(X[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttvcC9cnB6yK"
      },
      "outputs": [],
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layers):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CclBWWUsB_oW",
        "outputId": "b596bc45-cb83-42b3-8a97-8c2cdc8d0141"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rprrgrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrprrrrrrrprrrrrrrrrrrrrrrrrrrrgrprrrrrrrrrrrrrrprrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrprrrrr\n",
            "  t                         t                             t               t                t    t     t                    t          t           t           t            t       \n",
            "                                                                                                                                                                                   \n",
            "tib..n of.,b....e.....,.,.nbt.,.eht.,hnb....,..,.....,tf.,ba...nb.,b.....ib.....,.eb..,.e.,b...,b.....i.,...,.,bb...e.,.i',b....eb.,.,.....nb.......,.i',.es,.....tb.........eb,.,.\n",
            "tehehihehiiiieihiieihiihiiihihehihiiihiiehihieiihiieiiiihiheeieehiihiihiiiiiiihieiiiiiihiiihiiiiiehihihiihieiiiiiiihiiiiihihehihihiihiieeihehiiheiieiiihiiehihiiiiieiiihieiieiiiiii\n",
            "tortntttnttttetnttetntttttenttenttttenentnttteettttenttentttettenttnttteettettntettttttntntttetettntnttetntteetttetntetteetttntttnttnettetnenetneettetttttentntteettttttteettettttt\n",
            "tstlololollllllllllllllllllllllllollllllllllllllllllllllllllsllslllllllsllllllllllllllllllllllllllolllllllllslllllllllllllllllltllllllllsllsllllllllllllllllltlllllllllllllllllllll\n",
            "tntol tnl ooon oonooonooono oon  ooon noon nono ooono oono on onooooooono onnooon  oonoono ooooo ononoonnooon  oonooonohono onotoooono on onooonn  ooo nnoooooonn onnoooono oon ono\n",
            "tolost olttooot oo ttotoo  ttoo toooo  toot    to oo ttoo tootto totoooo ttoottoottooo    ttoo  toot  toottoo t tootto too too to to  toottotttoo too tootto tto ttoottooo too too \n",
            "o  tntoo tttt tt  ttt t   ttto ttttt t to t  tttttt ttt  tto tt ttttttt ttt  tto ttte  tt ttt  tto tttto tto tttoo tt  t  tto tt tt  tto tt ttto tto  t  tt ttto tt ttttt  t   t  t\n",
            "           t n     t       t  n t              t                 t     n    n t n t        t          t n t n         st                        n t     n        t         t       \n",
            "           t n  n    ne e     n   e     en  e      n        n  n   e   n   on t n   e      t n  t n   t n t n     n   e  n   n        t n  n    n   n   n  n     t n   e   t n   n \n",
            " eto   e   ton  ene eee e e ton  eeene eoe  e   e on  eee t n  n  eee  n  een ton  eene ee  eee toe e ten ton   toe t e ee t n  e  e  ton en   on toe   n  e   n  en  ee n tee   ee\n",
            "  to  to   to       to      ton   a    ton      o on   oo ton     d o  n  to  ton   a      too  ton   ton ton   ton t d    t n  o  n  ton     wo  tor  o  t    o      o    t o   n \n",
            "  to  to   too  d d tna     ton toa    a  to     wo    oo to      d oa n  to  to  toa d o  too  too   tod ton   wod tod od ton  o     too     ao  too  o  to  ao  o  aot   too  on \n",
            "  to  ta   aon    d ao o  d ton toco   aoddo  t d o ctaad to dor e   aon  ao  to 'toto  o dtoa  tod n tod ton d to  tod ao tod   co   to  or  wor toa to  ao tao  o  oatao tondtod \n",
            "  ton tod t o to   ttn   ' t o 'tooo t to to   e don tt e toe' rdtnt ton'two tdo ' oon h ' toe' ao d 'to taon' nto  to 'od toot ed  ' wo 'ondtdon'too todtnn'ttocton d ton tod to  \n",
            " nton ton  ao  o oe ao o on ao 't to   ao  o     ton  oe  torten en  wo ' an  aon't to  n  ton  to  n to  to '  ao  ao  e 'to    to   to  en  ton'to nton e  tao  n  aoth  to  ton \n",
            " ntoo tn  ttnnto  t tnt e e ton'teto t tontn  nnttonctae  tantontent ton tan  ann'thtn  nn'ta   tn tn tnntton t to  to  ei ta taetoee annten' ton toeetn  n  tan  n tnnta  to  to  \n",
            " nton tost ao to  n t the n t n't bo t aonto     ton tten thnto tent to   on  aon'tebo  n 'then ton n ton ton t to  ao  en'thnto to n tonten' ton thenton e  to   nn eo o  toenton \n",
            "  thn tos  eo t     to he   t n t bo t ao t      ton   en th th  e s wo   t   ton t bo  e 'the  thn   to  to    to  eo  en'th  h bh   th te   to  to  tonte  to   en eo o  the ton \n",
            " ston toss ao toe   tothene tos't bo t ao ton  e to  ttes to to tent to   aos to 't bo ten'the  toss  tos to  t toe aot en tosto to   to to ' to  the tos e ttae tes eoto  toe thst\n",
            " ston  ont to tot s tobhens tos't to t aoetondtelto ltoes to to tent to d ans ton't bostesstoes tosss tos tosdt tos tsthenstostoetoe  tostond to  toe tos esttte tns edto stoe tost\n",
            " ston tons to to  s t t e l ton't bo   ao t n    th  thes th to te t to l ans ton't bn   n'the  tos   tos to    to  ast e  th th toe  to to ' to  toe  os e t be   s esta  toe  hss\n",
            " rton toss to to  n t b e   t n't bo   ao t n    th   her th to    s to   ans t n't bs   n' he  tos   tns ton   to  ars e  to sh boe  th to ' to  toe  os e   a    s  oea  toe  ons\n",
            " ntod toss to to  a a d ers ton't dors ao t n  erth  ther th to lend to   and ton't dns  nsthem tosms tns aor s ao  ans er thrsh ioe  to tor' tor toemtos e d ae  ns  oea  toemtors\n",
            " nton tosd tortod n todhent don't dond aodtoc terthnlthen thrto lent tonl and ton't dns es'them tosps tns ton , aod anshen thrtoetoem torton' aoe toemtnd ncthae  ns toea  toemtors\n",
            " ntod tond to toi n d dhent don't dor  aoeten tenthnlthen th to lent to l and ton't dndten'thep thsps tns ton , to  anther thrtoetoem th ton' toe themtnd ecd ae  nd toea  themthnt\n",
            " ntod ton, eo toe e d dhept don't der  ao per  enth  then th eo lent to l and ton't dn   n'the  th es tnd ton , to  enther th theihe  th eo ' woe thep os e   a   nd  oea  thep  nt\n",
            " ntod ton, ao toi d d dhe t don't dont aoecen  enth lthen th to lent to l and don't dndt n'the  thses tnd ton , tod anthen thntoetoe  th eo ' toe the tns e   aed ns  oeo  themtont\n",
            " ntod won, to tod d d dhe t don't dont aoeden tento lthen th to lent to l ond don't dndt n'them tosts tnd ton , dod anthen thntoetoeu to to , tos toemtns nnt a   nstto on toemtont\n",
            " ntod tond to to  d d dte t don't dout ao per  e to lthem to to eert to l and don't dsst ngthe  ths s tnd ton , tot aothe  torto toe  th to g to  toemtns e t a   nstto o  toemtont\n",
            " ntor tond to to  d d dhe , don't dout ae per  e to lthe  to to lert to l and don't dsst n the  tos sttns tor , dot authe  torth toe  to to g to  toem ns e sti   nsttd o  toem eus\n",
            " ator tond wo tob d d dhe , don't dout ae peu le to lther to to lert to d and don't dnst n them tosssttns wor , dot auther toushetoem to torg tor toem ns e shi   usttd a  toem ens\n",
            " atod tond wo tos d d she , don't doum ao pen le to lther th to lert to l and won't dnst n them tosssttns wor , wot ausher thrsh them th torg tor them ns e shr   ustta or them ens\n",
            " atod tond wo tos d d sher, don't doum ap pea le to lther th lo lert to dlant won't dsst n them thsss tns wor , wot ausher thrsh them wh lorg tor themlas e shr   nsita or themtens\n",
            " rton tond wo tos d d aher, don't dpum ap pen lecto lther th lo lert ao dlant don't dssi n them ths s tnd wor , wom ausher thash them th lorg wor themtas ess r   asi a or themteas\n",
            "  tod tond th trs d d sher, don't apum ap pea le to lther th lo lert ao d and don't assi n them ths s tnd wor , wom aasher thash them th long ior themins esshr   ns th or themteas\n",
            "  tou tond to trs d d aher, don't apum aempenple to lthem th lo lert ao d ant don't assi n them ths s tnd wor , domhaasher thash them to long ior themtns ess amm ns ta or themtens\n",
            "  tod tont to bri d d shep, don't dpum ae penple to ether to lo lert to d ant don't dnsign them tos s tnd wor , dom auther toash them to long ior toe tnd ess a m n' ty on toemtens\n",
            "  tod tont to tui d d shep, don't dpum ae peaple to lther to  o lect to d ant don't dnsign them tos s and work, dum iuther toash the  to tong por toe end ess a men' ty on toemeens\n",
            "l tou tont to tui d d sher, don't dpum ne penple to ether to to lect tord ant don't dtsign them tosks and work, duc iuther tonsh them to torg tor toemendless ammens ty or toemeest\n",
            "l tou tont to cum d d sher, don't deum no peaple to lther to co eect to d ant don't dtsign them tosks and work, duc iuther tonch them to co g por toemendless nmmens ty or toemeesc\n",
            "l toumtont to bui d d sher, don't deum no penple to ether to lo lect wordlant don't dtsign them tosks and work, dut iuther tonch them to lorg tor themensless nmmens ty or toemeesc\n",
            "l tou tont to lui d d sher, don't deum ur peaple to ether th lo lect wood ant don't dtsign them tosks and work, dut iuther thnch them to long por themendless im ens ty or toemeenc\n",
            "l tou tont to lui d d sher, don't drum ue peaple to ether th lo lect wo d ant don't dssign them tosks and work, dut iuther thach them to long por the endless im ens ty or toemeens\n",
            "lrtou wont to lui d a ship, don't arum ue peaple to ether th lo lect wood and don't ansign them tasks and dork, dut iuther thach ther to long por themendless immensity or toemeens\n",
            "lmtou wont to bui d a shep, don't drum rp peaple to ether th lo lect wood and don't ansign them tosks and work, wut iather thach them to bong por themendless immensity or themeeac\n",
            "lmtou want to but d a ship, don't aoum up peaple to ether th lo lect wood and don't ansign them tasks and work, wut iather thach them ta bong tor themendless immensity or themeeac\n",
            "lriou wont to build a ship, don't arum re pemple to ether th lo lect wood and don't atsign them tasks and work, dut iather thach them to borg tor themendless immensity or themeeac\n",
            "l tou want th luild a ship, don't drum up peaple to ether th lo lect wood and don't dtsign them tosks and work, dut iather thach them to long tor themendless immensity or themeeac\n",
            "l tou want th build a ship, don't drum up peaple to ether th lo lect word and don't dtsign them tasks and work, dut iather thach them ta bong for themendless immensity or themeeac\n",
            "l tou want to build a ship, don't arum up peaple to ether th co lect wood and don't atsign them tosks and work, dut iather thach them to cong for themendless rmmensity or themeeac\n",
            "l tou want th build a ship, don't arum up peapee to ether th lo lect wood and don't atsign them tosks and work, dut iather thach them to bong for themendless immensity of themeeac\n",
            "l tou want to build a ship, don't arum up peaple to ether th to lect wood and don't atsign them tosks and work, wut rather thach them ta long for toemendless immensity of themeeac\n",
            "l tou want to build a ship, don't drum up people together th to lect word and don't dtsign them tosks and work, dut rather thach them to long tor toemendless immensity of themeeac\n",
            "l tou want to luild a ship, don't drum up people to ether th to lect wood and won't dtsign them tosks and work, wut iather thach them to long for themendless immensity of themeeac\n",
            "l tou want to build a ship, don't drum up people together to lollect wood and don't dnsign them tosks and work, dut iather thach them to long for toemendless immensity of toemeeac\n",
            "l tou want th luild a ship, don't drum up people together th lo lect wood and don't atsign them tosks and work, tut iather thach the  to long for the endless immensity of the eeas\n",
            "l iou want to cuild a ship, don't drum up people to ether th collect word and don't dtsign them tosks and work, dut iather thach them ta long for themendless immensity of toemeeac\n",
            "l tou want th lumld a ship, don't drum up people together th lollect wood a d don't atsign them tosks and work, dut uather thach them ta long tor themendless immensity of the eeac\n",
            "l iou want to build a ship, don't drum up people together th toglect wood and don't dnsign them tosks and work, dui iather toach them to borg for themendless immensity of toemeeac\n",
            "l iou want to build a ship, don't arum up peaple to ether th coglect wood and don't ansign toem tasks and work, dui iather toach them ta bong for themendless immensity of themeeac\n",
            "l toa want to build a ship, don't drum up people to ether th collect wood and won't dssign them tasks and work, dut rather thach them ta bong for themeudless immensity of the e ac\n",
            "l iou want to build a ship, don't drum up people to ether th collect wood and don't dssign them tosks and work, dut rather thach them ta long for themendless immensity of themeeac\n",
            "l iou want to build a ship, don't drum up people together th collect wood and don't dssign them tosks and work, dut rather thach them to long for themendless immensity of themeeas\n",
            "l iou want to luild a ship, don't drum up people together th loglect wood and don't dnsign them tosks and work, dut rather thach them to long for themendless immensity of the eeas\n",
            "l iou want toobuild a ship, don't drum up people together th lollect wood and don't dssign them tosks and work, dut ratheo thach them to long for themendless immensity of the eeas\n",
            "l iou want to cuild a ship, don't drum up people together th collect wood and don't dssign them tosks and work, dut rather thach them to cong for themendless immensity of the eeas\n",
            "l iou want to luild a ship, don't drum up people together th collect word and don't dssign them tasks and work, dut rather thach the  ta long for the endless immensity of the eeac\n",
            "luiou want to build a ship, don't arum up people together th collect wood and don't assign the  tasks and work, dut rather thach the  ta long for the endless immensity of the eeac\n",
            "l ieu want to build a ship, don't arum up people together th collect wood and don't assign them tasks and work, dut rather thach them ta long for the endless immensity of the eeac\n",
            "l iou want to build a ship, don't arum up people together th collect wood and won't assign them tasks and work, wut rather thach them ta long for themendless immensity of the eeas\n",
            "l iou want to luild a ship, don't drum up people together th collect wood and don't dssign them tasks and work, wut rather thach them ta long for the endless immensity of the eeas\n",
            "l iou want to build a ship, don't drum up people together th collect wood and don't dssign them tosks and work, wut rather thach them to long for the endless immensity of thereeas\n",
            "l iou want to build a ship, don't drum up people together th collect wood and don't dssign them tosks and work, dut rather thach them to long for themendless immensity of the eeas\n",
            "l iou want to build a ship, don't drum up people together th collect wood and don't dssign them tosks and work, but rather thach them to long for themendless immensity of themeeas\n",
            "l you want to build a ship, don't drum up people together th collect wood and don't dssign them tasks and work, but rather thach them ta long for themendless immensity of themeeas\n",
            "l you want to build a ship, don't drum up people together th collect wood and don't dssign them tasks and work, but rather thach the  ta long for the endless immensity of the eeas\n",
            "l you want to build a ship, don't arum up people together th collect wood and don't assign them tosks and work, but rather toach the  to long for the endless immensity of the eeas\n",
            "l you want to build a ship, don't arum up people together th collect wood and don't assign them tosks and work, but rather toach the  to long for the endless immensity of the eeas\n",
            "l you want to build a ship, don't drum up people together th collect wood and don't dssign them tosks and work, but rather teach the  to long for the endless immensity of the eeas\n",
            "g you want to build a ship, don't drum up people together te collect wood and don't dssign them tasks and work, but rather teach the  ta long for the endless immensity of the eeas\n",
            "f you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach the  to long for the endless immensity of the eeac\n",
            "f you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach the  to long for the endless immensity of the eeac\n",
            "l you want to build a ship, don't drum up people together te collect wood and don't dssign them tasks and work, but rather teach the  ta long for the endless immensity of the eeac\n",
            "l you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the eeac\n",
            "l you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the eeac\n",
            "l you want to build a ship, don't drum up people together te collect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immensity of the eeac\n",
            "l you want to build a ship, don't drum up people together te collect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immensity of the eeac\n",
            "l you want to build a ship, don't arum up people together te collect wood and don't assign them tosks and work, but rather teach the  to long for the endless immensity of the eeac\n",
            "l you want to build a ship, don't arum up people together te collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the eeas\n",
            "l you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the seas\n",
            "g you want to build a ship, don't arum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the eeas\n",
            "g you want to build a ship, don't arum up people together te collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the eeas\n",
            "g you want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
            "g you want to build a ship, don't drum up people together te collect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immensity of the eeas\n",
            "g you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the eeas\n",
            "f you want to build a ship, don't arum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the eeas\n",
            "f you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the seas\n"
          ]
        }
      ],
      "source": [
        "net = Net(dic_size, hidden_size, 2) # 이번에는 층을 두 개 쌓습니다.\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
        "\n",
        "outputs = net(X)\n",
        "\n",
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # results의 텐서 크기는 (170, 10)\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
        "            predict_str += ''.join([char_set[t] for t in result])\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "            predict_str += char_set[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNncY/oXpz7HQ9TqTYWp9bO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}